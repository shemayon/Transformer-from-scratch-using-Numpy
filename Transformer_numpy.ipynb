{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step by step implementation of Transformer model from the scratch using Numpy**"
      ],
      "metadata": {
        "id": "GglEZxCwnCon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import libraries**\n",
        "\n",
        "libraries for data handling, text preprocessing, and embedding creation"
      ],
      "metadata": {
        "id": "g1p6lD0inVKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure you have the stopwords corpus\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "3A5JHRtisHMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8beeae-289a-4b10-ce91-8ec70b08b24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data loading and Pre-processing**\n",
        "\n",
        "loading the data from a CSV file and preparing it for training by creating source and target columns"
      ],
      "metadata": {
        "id": "REYnIbPpfE-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = r'/content/sample_data/en-fr.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df['source'] = df['English words/sentences']\n",
        "df['target'] = df['French words/sentences'].apply(lambda x: '[start] ' + x + ' [end]')\n",
        "df = df.drop(['English words/sentences', 'French words/sentences'], axis=1)\n",
        "\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgloTRyrfGj3",
        "outputId": "d2c8f06b-ad70-4692-8b75-036d560554e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  source                    target\n",
            "0    Hi.      [start] Salut! [end]\n",
            "1   Run!     [start] Cours ! [end]\n",
            "2   Run!    [start] Courez ! [end]\n",
            "3   Who?       [start] Qui ? [end]\n",
            "4   Wow!  [start] Ça alors ! [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle the data and split it into training, validation, and test sets"
      ],
      "metadata": {
        "id": "YS65o3Xoff5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Q_ZFrUdrc9CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(df) * 0.7)\n",
        "val_size = int(len(df) * 0.2)\n",
        "test_size = int(len(df) * 0.1)\n",
        "\n",
        "print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")  # Check split sizes\n",
        "\n",
        "train_df = df[:train_size]\n",
        "val_df = df[train_size:train_size + val_size]\n",
        "test_df = df[train_size + val_size:]\n",
        "\n",
        "print(f\"Train set size: {len(train_df)}, Validation set size: {len(val_df)}, Test set size: {len(test_df)}\")  # Verify dataset splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc6_AE3Oc-MJ",
        "outputId": "507b8ac9-602e-47bf-b5ea-3083e626b5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 122934, Val size: 35124, Test size: 17562\n",
            "Train set size: 122934, Validation set size: 35124, Test set size: 17563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the text by converting to lowercase, removing punctuation, and removing stopwords and very short tokens"
      ],
      "metadata": {
        "id": "o7lrLeOvgWR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess sentences to normalize text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    # Convert to lowercase\n",
        "    sentence = sentence.lower()\n",
        "    # Remove punctuation (except for tokens like [start] and [end])\n",
        "    sentence = re.sub(r'[^a-z0-9\\s\\[\\]]', '', sentence)\n",
        "    # Remove stop words and very short tokens\n",
        "    sentence = ' '.join([word for word in sentence.split() if word not in stop_words and len(word) > 2])\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "UcTTm9aIdCuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "train_df['source'] = train_df['source'].apply(preprocess_sentence)\n",
        "train_df['target'] = train_df['target'].apply(preprocess_sentence)\n",
        "val_df['source'] = val_df['source'].apply(preprocess_sentence)\n",
        "val_df['target'] = val_df['target'].apply(preprocess_sentence)\n",
        "test_df['source'] = test_df['source'].apply(preprocess_sentence)\n",
        "test_df['target'] = test_df['target'].apply(preprocess_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6WOcORZdIQ0",
        "outputId": "d97e7070-4c0c-485b-cc14-3cfe5fb57671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b4ff71ef4469>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['source'] = train_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['target'] = train_df['target'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df['source'] = val_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df['target'] = val_df['target'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['source'] = test_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['target'] = test_df['target'].apply(preprocess_sentence)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())  # Verify preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKfVTxl-gmJj",
        "outputId": "9668a335-63dd-4468-b161-37d8ec3d9a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  source                                             target\n",
            "0                    let                [start] laissemoi faire place [end]\n",
            "1             call party                      [start] jai annuler fte [end]\n",
            "2                   want               [start] veux parcourir nouveau [end]\n",
            "3         world come end      [start] quand monde connatratil une fin [end]\n",
            "4  owe part success luck  [start] nous devons une part notre succs chanc...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Vocabulary**\n",
        "\n",
        "create a simple vocabulary from the training data"
      ],
      "metadata": {
        "id": "iHfJjDYrdPeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab(sentences):\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        vocab.update(sentence.split())\n",
        "    vocab = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab\n",
        "\n",
        "# Create vocabulary from the training data\n",
        "vocab = create_vocab(train_df['source'].tolist() + train_df['target'].tolist())\n",
        "vocab_size = len(vocab)\n"
      ],
      "metadata": {
        "id": "4B9dng_pdOXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Input Embedding**\n",
        "\n",
        "Initialize the embedding matrix with random values and create functions to get word embeddings"
      ],
      "metadata": {
        "id": "P_qffB0ghC1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 512\n",
        "\n",
        "# Initialize the embedding matrix with random values\n",
        "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")  # Verify shape of embedding matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_yrcvF7dZY9",
        "outputId": "2d8254a6-42fe-44a8-f4c2-907196853eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (35203, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, vocab, embedding_matrix):\n",
        "    idx = vocab.get(word, -1)\n",
        "    if idx == -1:\n",
        "        raise ValueError(f\"Word '{word}' not in vocabulary.\")\n",
        "    return embedding_matrix[idx]\n"
      ],
      "metadata": {
        "id": "T4y6idI7dfRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Positional Encoding**\n",
        "\n"
      ],
      "metadata": {
        "id": "LV2KFOh_p8Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create positional encoding\n",
        "def get_positional_encoding(max_len, embedding_dim):\n",
        "    position = np.arange(max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n",
        "    pos_encoding = np.zeros((max_len, embedding_dim))\n",
        "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "    return pos_encoding"
      ],
      "metadata": {
        "id": "MrWoZTw7pdMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Combine Embeddings and Positional Encodings**\n",
        "\n",
        "combine word embeddings and positional encodings for a given sentence"
      ],
      "metadata": {
        "id": "HS_-4i-5dwEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = train_df['source'].iloc[0].split()\n",
        "sentence_len = len(sentence)\n",
        "print(f\"Sentence: {sentence}, Length: {sentence_len}\")  # Verify sentence and length\n",
        "\n",
        "sentence_embeddings = np.array([get_embedding(word, vocab, embedding_matrix) for word in sentence])\n",
        "print(f\"Sentence Embeddings shape: {sentence_embeddings.shape}\")  # Verify embeddings shape\n",
        "\n",
        "positional_encodings = get_positional_encoding(sentence_len, embedding_dim)\n",
        "print(f\"Positional Encodings shape: {positional_encodings.shape}\")  # Verify positional encodings shape\n",
        "\n",
        "# Add input embeddings and positional encodings\n",
        "input_embedding_with_position = sentence_embeddings + positional_encodings[:sentence_len, :]\n",
        "print(f\"Combined Embedding and Positional Encoding shape: {input_embedding_with_position.shape}\")\n",
        "print(f\"Combined Embedding and Positional Encoding:\\n{input_embedding_with_position}\")"
      ],
      "metadata": {
        "id": "-f2w5f4Vutjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a6aac9-9d36-4dd5-946b-003aeca3d0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ['let'], Length: 1\n",
            "Sentence Embeddings shape: (1, 512)\n",
            "Positional Encodings shape: (1, 512)\n",
            "Combined Embedding and Positional Encoding shape: (1, 512)\n",
            "Combined Embedding and Positional Encoding:\n",
            "[[1.73731108e-01 1.90352661e+00 4.58789950e-02 1.41171438e+00\n",
            "  2.35262215e-01 1.89901722e+00 9.75038506e-02 1.02556805e+00\n",
            "  4.93127897e-01 1.34464184e+00 7.38890157e-01 1.38065074e+00\n",
            "  9.29246763e-01 1.07834625e+00 5.59320069e-01 1.10910912e+00\n",
            "  4.06384533e-01 1.79570402e+00 6.51046344e-01 1.59282110e+00\n",
            "  1.21720618e-01 1.14286398e+00 7.67434200e-02 1.38213775e+00\n",
            "  1.50402843e-01 1.34742302e+00 1.68151550e-01 1.06144327e+00\n",
            "  2.21312866e-01 1.57477523e+00 6.93169151e-02 1.24039111e+00\n",
            "  4.21081651e-01 1.23549873e+00 6.55963583e-01 1.45318278e+00\n",
            "  8.92497455e-01 1.29214904e+00 3.82383566e-01 1.07382502e+00\n",
            "  4.24003743e-01 1.67548654e+00 7.61881782e-01 1.58933606e+00\n",
            "  9.83268641e-01 1.85585367e+00 4.34684264e-01 1.50688094e+00\n",
            "  9.39106900e-01 1.59592973e+00 1.90595739e-01 1.65637607e+00\n",
            "  3.36462282e-01 1.71898405e+00 7.95712922e-01 1.60776862e+00\n",
            "  5.36306934e-01 1.78079527e+00 9.91497698e-02 1.61895098e+00\n",
            "  7.00009111e-01 1.89587367e+00 5.39438807e-01 1.26560812e+00\n",
            "  4.83933543e-01 1.59425464e+00 4.82000187e-01 1.05938541e+00\n",
            "  2.15625653e-01 1.30929383e+00 8.31191953e-01 1.59324168e+00\n",
            "  3.16413209e-01 1.79585353e+00 2.80754220e-01 1.52407174e+00\n",
            "  5.69422216e-01 1.24732061e+00 7.62876366e-01 1.12457696e+00\n",
            "  5.00746389e-01 1.28181439e+00 5.99285315e-01 1.63124877e+00\n",
            "  7.03098168e-03 1.75808024e+00 6.09642334e-01 1.33347076e+00\n",
            "  8.66795028e-01 1.20889915e+00 3.03305607e-01 1.24217097e+00\n",
            "  7.97282995e-01 1.42655392e+00 7.14034282e-01 1.53663145e+00\n",
            "  5.93006810e-01 1.79177176e+00 7.04510456e-02 1.65199673e+00\n",
            "  3.41273815e-01 1.04152876e+00 4.19176780e-01 1.03754819e+00\n",
            "  5.90318136e-01 1.29939158e+00 3.27758519e-01 1.61825774e+00\n",
            "  6.64543919e-01 1.53926255e+00 9.44105896e-02 1.79858148e+00\n",
            "  2.51902245e-01 1.15541937e+00 4.94863595e-02 1.86418920e+00\n",
            "  7.87285757e-02 1.07198818e+00 2.87531872e-01 1.46284409e+00\n",
            "  4.28279655e-01 1.44127325e+00 4.51472889e-01 1.61235059e+00\n",
            "  4.02864252e-01 1.24367796e+00 2.47879516e-01 1.97059391e+00\n",
            "  6.01580753e-01 1.14697210e+00 1.45810402e-01 1.66644719e+00\n",
            "  4.61014503e-01 1.84022542e+00 7.02910736e-01 1.32709218e+00\n",
            "  1.22949393e-01 1.46539988e+00 6.90754271e-01 1.56618819e+00\n",
            "  7.92257409e-03 1.67096951e+00 7.35168064e-01 1.65867035e+00\n",
            "  5.09560273e-01 1.04629709e+00 7.89983379e-01 1.69652143e+00\n",
            "  8.91828221e-01 1.95212524e+00 5.76649493e-01 1.03213696e+00\n",
            "  5.03104277e-01 1.33575780e+00 7.76943876e-01 1.57632779e+00\n",
            "  2.66350294e-01 1.27526852e+00 4.28776136e-01 1.58305309e+00\n",
            "  1.44339294e-01 1.31032479e+00 6.75015053e-01 1.27684263e+00\n",
            "  8.00342043e-01 1.55272867e+00 2.51211134e-01 1.22293953e+00\n",
            "  1.16360914e-03 1.02291120e+00 2.55796751e-01 1.58279644e+00\n",
            "  5.11357949e-01 1.17987848e+00 7.76168169e-01 1.70980661e+00\n",
            "  7.94026755e-02 1.33912144e+00 6.98133123e-01 1.42030681e+00\n",
            "  6.92898794e-01 1.67437076e+00 9.57749117e-01 1.97173242e+00\n",
            "  8.91415307e-01 1.99220448e+00 2.39164887e-01 1.90387273e+00\n",
            "  7.73378669e-01 1.82067247e+00 4.16282328e-01 1.28036375e+00\n",
            "  7.96713402e-01 1.82378793e+00 4.46052325e-01 1.17765497e+00\n",
            "  7.09905718e-01 1.88633766e+00 1.73695947e-01 1.27479302e+00\n",
            "  1.54299239e-01 1.02421015e+00 7.62832916e-01 1.52456354e+00\n",
            "  9.93390755e-01 1.01270480e+00 4.30891428e-01 1.72252018e+00\n",
            "  1.79873737e-01 1.66315220e+00 9.30554483e-01 1.61124026e+00\n",
            "  4.09275405e-01 1.75877063e+00 6.28915400e-01 1.06120824e+00\n",
            "  2.17160677e-01 1.48688290e+00 7.89355234e-01 1.71813747e+00\n",
            "  5.06055909e-01 1.56929670e+00 4.05024795e-01 1.15525514e+00\n",
            "  8.43248232e-01 1.01134048e+00 2.98551310e-01 1.73398745e+00\n",
            "  2.02670602e-01 1.81331508e+00 9.83060160e-01 1.56387941e+00\n",
            "  1.57867074e-01 1.10991120e+00 4.22705258e-01 1.34892782e+00\n",
            "  7.26141649e-02 1.73178418e+00 1.04854976e-01 1.93876373e+00\n",
            "  1.75421604e-01 1.02986620e+00 6.12262656e-01 1.91564582e+00\n",
            "  8.81223545e-01 1.19077056e+00 8.49357602e-01 1.11627865e+00\n",
            "  7.45740565e-01 1.11030159e+00 9.84120478e-01 1.16416436e+00\n",
            "  3.31608508e-02 1.46532735e+00 2.21183972e-01 1.64007787e+00\n",
            "  5.87525600e-01 1.67051285e+00 5.43308391e-01 1.79382749e+00\n",
            "  4.24198561e-01 1.56853690e+00 9.59558358e-01 1.73354816e+00\n",
            "  3.69249627e-01 1.45115292e+00 1.82976726e-01 1.86674581e+00\n",
            "  1.43596046e-01 1.01117149e+00 3.85072929e-01 1.48753593e+00\n",
            "  5.82395476e-01 1.44317372e+00 9.72159444e-01 1.65469954e+00\n",
            "  8.79203927e-01 1.37155669e+00 6.68603621e-01 1.37950917e+00\n",
            "  7.87092569e-02 1.27761132e+00 7.30096806e-01 1.10671092e+00\n",
            "  7.12914659e-01 1.68686562e+00 1.35584903e-01 1.40989369e+00\n",
            "  2.93397585e-01 1.47943897e+00 8.43758246e-01 1.20457571e+00\n",
            "  8.67383170e-01 1.32871509e+00 9.89876471e-01 1.29501872e+00\n",
            "  8.68676479e-01 1.60152366e+00 2.37089229e-01 1.14569004e+00\n",
            "  9.54629628e-01 1.86143160e+00 6.17492697e-01 1.35826746e+00\n",
            "  5.21696290e-01 1.64699248e+00 2.79739929e-01 1.31683251e+00\n",
            "  6.62646740e-01 1.08990899e+00 5.55955292e-01 1.66917102e+00\n",
            "  8.56536275e-01 1.93104815e+00 9.83581953e-01 1.50719058e+00\n",
            "  4.63599804e-01 1.53623835e+00 3.26396760e-01 1.97992388e+00\n",
            "  5.50827294e-01 1.75702308e+00 5.31159549e-01 1.45616232e+00\n",
            "  5.96856740e-01 1.84859562e+00 3.65379554e-01 1.58023859e+00\n",
            "  8.53690445e-01 1.03020428e+00 1.13654652e-01 1.43510341e+00\n",
            "  8.42827195e-01 1.13746013e+00 1.45905760e-01 1.83088249e+00\n",
            "  4.12494716e-01 1.85373425e+00 9.56495274e-01 1.18343028e+00\n",
            "  4.41321971e-01 1.43651253e+00 4.45350431e-01 1.99897948e+00\n",
            "  2.49144227e-01 1.87158612e+00 7.49376733e-01 1.68126220e+00\n",
            "  4.30094128e-01 1.88759937e+00 1.49268741e-01 1.93620835e+00\n",
            "  1.85715261e-01 1.24100911e+00 1.70464306e-01 1.53589789e+00\n",
            "  2.45060890e-01 1.22526022e+00 2.63220764e-01 1.89832230e+00\n",
            "  9.71851042e-01 1.04576005e+00 2.35902544e-01 1.24926038e+00\n",
            "  9.85257553e-01 1.54514821e+00 6.65875209e-01 1.19713171e+00\n",
            "  5.80507005e-01 1.95540547e+00 7.88431192e-01 1.40489416e+00\n",
            "  2.00046467e-01 1.73022318e+00 8.14258427e-01 1.60587292e+00\n",
            "  4.20912735e-01 1.02351936e+00 5.87942946e-01 1.02665588e+00\n",
            "  4.47615640e-03 1.16697984e+00 4.48208974e-01 1.66667020e+00\n",
            "  5.28806530e-02 1.17535521e+00 7.18850763e-01 1.21347823e+00\n",
            "  4.73293753e-01 1.11306715e+00 7.67477291e-01 1.93031727e+00\n",
            "  5.20183554e-01 1.50667627e+00 9.70375641e-01 1.39903071e+00\n",
            "  2.90392666e-02 1.12146211e+00 1.66593816e-01 1.47267830e+00\n",
            "  5.67349902e-01 1.41438891e+00 3.38591093e-01 1.57189289e+00\n",
            "  9.48057052e-01 1.27341642e+00 8.64120391e-01 1.47114679e+00\n",
            "  9.55669539e-01 1.17559583e+00 9.47607831e-01 1.71217370e+00\n",
            "  8.67078706e-01 1.38718510e+00 1.25936270e-02 1.27359691e+00\n",
            "  7.09922804e-01 1.67757183e+00 8.28548908e-01 1.66298300e+00\n",
            "  6.66791108e-02 1.87663816e+00 3.29885895e-01 1.39662660e+00\n",
            "  3.73100538e-02 1.30006005e+00 4.74080641e-01 1.36497117e+00\n",
            "  1.38831994e-01 1.29100031e+00 7.93430130e-01 1.47978592e+00\n",
            "  9.67499236e-01 1.17091376e+00 5.71761130e-01 1.54395237e+00\n",
            "  5.88182524e-02 1.59974568e+00 9.58781916e-01 1.70040630e+00\n",
            "  9.49729226e-01 1.15131246e+00 1.82699817e-01 1.21502807e+00\n",
            "  2.59647611e-01 1.88543844e+00 8.99742475e-01 1.56525388e+00\n",
            "  1.75186695e-02 1.90677272e+00 7.66032582e-01 1.47229486e+00\n",
            "  2.34054614e-01 1.48514010e+00 2.48196932e-01 1.82948344e+00\n",
            "  2.14260391e-01 1.68652155e+00 2.16960558e-01 1.13547331e+00\n",
            "  9.54024891e-01 1.59393705e+00 1.62700655e-01 1.09761167e+00\n",
            "  3.98718870e-03 1.01995113e+00 9.25083375e-01 1.13441575e+00\n",
            "  5.55980694e-01 1.42372775e+00 6.02573933e-02 1.10862389e+00\n",
            "  8.56670394e-01 1.77625787e+00 7.81907532e-01 1.30901065e+00\n",
            "  4.67539977e-01 1.73994031e+00 1.74912173e-02 1.62322308e+00\n",
            "  2.33344549e-01 1.31367113e+00 8.03101990e-01 1.46085687e+00\n",
            "  5.45219274e-01 1.63358890e+00 7.13883113e-01 1.23178064e+00\n",
            "  1.59560066e-01 1.61181692e+00 6.24698014e-01 1.00224363e+00\n",
            "  3.19400608e-01 1.14965067e+00 1.60390258e-01 1.60733067e+00\n",
            "  9.98943892e-01 1.06665282e+00 4.23264779e-01 1.79650291e+00\n",
            "  8.55308071e-01 1.76550008e+00 8.51716128e-01 1.25649857e+00\n",
            "  4.73998762e-01 1.16791230e+00 8.49612389e-01 1.45778858e+00\n",
            "  2.04023685e-01 1.20416891e+00 6.24429104e-01 1.29947872e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOxPsSIJut-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder**"
      ],
      "metadata": {
        "id": "1EsPNtr5R_Cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer normalization layer"
      ],
      "metadata": {
        "id": "uyGEKfcGCfy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_normalization(x, epsilon=1e-6):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    normalized_x = (x - mean) / np.sqrt(variance + epsilon)\n",
        "    return normalized_x"
      ],
      "metadata": {
        "id": "zjdZfJ0jSCQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-head Self Attention"
      ],
      "metadata": {
        "id": "sam3Ot29GI2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_head_attention(query, key, value, num_heads):\n",
        "    d_k = query.shape[-1]  # Dimensionality of the key/query/value\n",
        "    assert d_k % num_heads == 0\n",
        "\n",
        "    # Split the embedding into multiple heads\n",
        "    def split_heads(x):\n",
        "        new_shape = x.shape[:-1] + (num_heads, d_k // num_heads)\n",
        "        x = x.reshape(new_shape)\n",
        "        return x\n",
        "\n",
        "    query = split_heads(query)\n",
        "    key = split_heads(key)\n",
        "    value = split_heads(value)\n",
        "\n",
        "    # Scaled Dot-Product Attention\n",
        "    def scaled_dot_product_attention(q, k, v):\n",
        "        matmul_qk = np.matmul(q, k.transpose((0, 1, 3, 2)))  # (batch_size, num_heads, seq_len, seq_len)\n",
        "        dk = q.shape[-1]  # d_k\n",
        "        scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "        attention_weights = np.exp(scaled_attention_logits - np.max(scaled_attention_logits, axis=-1, keepdims=True))\n",
        "        attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "        output = np.matmul(attention_weights, v)\n",
        "        return output\n",
        "\n",
        "    attention_output = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "    # Concatenate heads\n",
        "    def concatenate_heads(x):\n",
        "        new_shape = x.shape[:-2] + (d_k,)\n",
        "        x = x.reshape(new_shape)\n",
        "        return x\n",
        "\n",
        "    attention_output = concatenate_heads(attention_output)\n",
        "    return attention_output\n"
      ],
      "metadata": {
        "id": "ozxgYG5DGCK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward Network"
      ],
      "metadata": {
        "id": "GrvV_h5dGQ7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(x, d_ff, embedding_dim):\n",
        "    W1 = np.random.randn(embedding_dim, d_ff)\n",
        "    b1 = np.random.randn(d_ff)\n",
        "    W2 = np.random.randn(d_ff, embedding_dim)\n",
        "    b2 = np.random.randn(embedding_dim)\n",
        "\n",
        "    def feed_forward(x):\n",
        "        x = np.maximum(0, np.matmul(x, W1) + b1)  # ReLU activation\n",
        "        x = np.matmul(x, W2) + b2\n",
        "        return x\n",
        "\n",
        "    return feed_forward(x)\n"
      ],
      "metadata": {
        "id": "sBdlzeIlGQHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder block"
      ],
      "metadata": {
        "id": "hdda7lHnGg1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(x, num_heads, d_ff, embedding_dim, epsilon=1e-6):\n",
        "    # Self-Attention\n",
        "    attention_output = multi_head_attention(x, x, x, num_heads)\n",
        "    attention_output += x  # Residual connection\n",
        "    attention_output = layer_normalization(attention_output, epsilon)  # Add & Normalize\n",
        "\n",
        "    # Feed-Forward Network\n",
        "    ff_output = feed_forward_network(attention_output, d_ff, embedding_dim)\n",
        "    ff_output += attention_output  # Residual connection\n",
        "    ff_output = layer_normalization(ff_output, epsilon)  # Add & Normalize\n",
        "\n",
        "    return ff_output\n"
      ],
      "metadata": {
        "id": "GylUO-dbGNOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder stack"
      ],
      "metadata": {
        "id": "0wpa8tG8Go15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_stack(x, num_layers, num_heads, d_ff, embedding_dim, epsilon=1e-6):\n",
        "    for _ in range(num_layers):\n",
        "        x = encoder_layer(x, num_heads, d_ff, embedding_dim, epsilon)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "sXv7UuWuGmPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "embedding_dim = 512\n",
        "\n",
        "batch_size = 1\n",
        "sentence_length = 10\n",
        "sentence_embeddings = np.random.rand(batch_size, sentence_length, embedding_dim)\n",
        "\n",
        "# encoder stack with 6 encoder blocks\n",
        "encoded_output = encoder_stack(sentence_embeddings, num_layers, num_heads, d_ff, embedding_dim)\n",
        "print(f\"Encoded Output after 6 encoder layers:\\n{encoded_output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjZ6i2fGrS8",
        "outputId": "ae963660-1716-4475-b388-81fb8466ce5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Output after 6 encoder layers:\n",
            "[[[ 1.78587034 -0.89151299 -0.8893528  ... -1.75079118  0.06390319\n",
            "    1.07716843]\n",
            "  [ 1.79452838 -1.38380661 -1.18681588 ... -1.05053406  1.52258043\n",
            "    0.44015191]\n",
            "  [ 2.00914486 -2.2124666  -0.39139131 ... -1.74896168  1.29809918\n",
            "    0.31650239]\n",
            "  ...\n",
            "  [ 0.99193251 -1.32423736 -0.15863095 ... -1.99730583  0.74907146\n",
            "    1.24960147]\n",
            "  [ 0.92000762 -0.72016071 -0.85252363 ... -0.64731377  0.08072207\n",
            "    0.3547932 ]\n",
            "  [ 0.87252849 -1.5321061  -0.95942637 ... -1.83749966  0.78949547\n",
            "    0.80534251]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RnTuvJTHIXqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoder**"
      ],
      "metadata": {
        "id": "IsTAaucXLBzd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gsXfvkWjLFId"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}