{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step by step implementation of Transformer model from the scratch using Numpy**"
      ],
      "metadata": {
        "id": "GglEZxCwnCon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import libraries**\n",
        "\n",
        "libraries for data handling, text preprocessing, and embedding creation"
      ],
      "metadata": {
        "id": "g1p6lD0inVKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure you have the stopwords corpus\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "3A5JHRtisHMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34328e69-b94d-4bfa-e84d-174f16073347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data loading and Pre-processing**\n",
        "\n",
        "loading the data from a CSV file and preparing it for training by creating source and target columns"
      ],
      "metadata": {
        "id": "REYnIbPpfE-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = r'/content/sample_data/en-fr.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df['source'] = df['English words/sentences']\n",
        "df['target'] = df['French words/sentences'].apply(lambda x: '[start] ' + x + ' [end]')\n",
        "df = df.drop(['English words/sentences', 'French words/sentences'], axis=1)\n",
        "\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgloTRyrfGj3",
        "outputId": "4373214a-0906-45f2-927b-29fc77bce079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  source                    target\n",
            "0    Hi.      [start] Salut! [end]\n",
            "1   Run!     [start] Cours ! [end]\n",
            "2   Run!    [start] Courez ! [end]\n",
            "3   Who?       [start] Qui ? [end]\n",
            "4   Wow!  [start] Ça alors ! [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle the data and split it into training, validation, and test sets"
      ],
      "metadata": {
        "id": "YS65o3Xoff5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Q_ZFrUdrc9CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(df) * 0.7)\n",
        "val_size = int(len(df) * 0.2)\n",
        "test_size = int(len(df) * 0.1)\n",
        "\n",
        "print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")  # Check split sizes\n",
        "\n",
        "train_df = df[:train_size]\n",
        "val_df = df[train_size:train_size + val_size]\n",
        "test_df = df[train_size + val_size:]\n",
        "\n",
        "print(f\"Train set size: {len(train_df)}, Validation set size: {len(val_df)}, Test set size: {len(test_df)}\")  # Verify dataset splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc6_AE3Oc-MJ",
        "outputId": "c9ac9835-42ae-4d82-fe25-ac0b93fd37c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 122934, Val size: 35124, Test size: 17562\n",
            "Train set size: 122934, Validation set size: 35124, Test set size: 17563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the text by converting to lowercase, removing punctuation, and removing stopwords and very short tokens"
      ],
      "metadata": {
        "id": "o7lrLeOvgWR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess sentences to normalize text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    # Convert to lowercase\n",
        "    sentence = sentence.lower()\n",
        "    # Remove punctuation (except for tokens like [start] and [end])\n",
        "    sentence = re.sub(r'[^a-z0-9\\s\\[\\]]', '', sentence)\n",
        "    # Remove stop words and very short tokens\n",
        "    sentence = ' '.join([word for word in sentence.split() if word not in stop_words and len(word) > 2])\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "UcTTm9aIdCuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "train_df['source'] = train_df['source'].apply(preprocess_sentence)\n",
        "train_df['target'] = train_df['target'].apply(preprocess_sentence)\n",
        "val_df['source'] = val_df['source'].apply(preprocess_sentence)\n",
        "val_df['target'] = val_df['target'].apply(preprocess_sentence)\n",
        "test_df['source'] = test_df['source'].apply(preprocess_sentence)\n",
        "test_df['target'] = test_df['target'].apply(preprocess_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6WOcORZdIQ0",
        "outputId": "038f72fd-840d-4e77-cdf9-8cf4b9790cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b4ff71ef4469>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['source'] = train_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['target'] = train_df['target'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df['source'] = val_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df['target'] = val_df['target'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['source'] = test_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-6-b4ff71ef4469>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['target'] = test_df['target'].apply(preprocess_sentence)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())  # Verify preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKfVTxl-gmJj",
        "outputId": "fc327d4e-5932-4a7a-f189-f80b5d5d2c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       source  \\\n",
            "0  hes bit rough around edges   \n",
            "1                    much get   \n",
            "2   understand tom didnt want   \n",
            "3          hes worried result   \n",
            "4                  drank wine   \n",
            "\n",
            "                                              target  \n",
            "0                       [start] est peu revche [end]  \n",
            "1         [start] combien devraientils obtenir [end]  \n",
            "2  [start] comprends pourquoi tom voulait pas fai...  \n",
            "3                     [start] proccupe rsultat [end]  \n",
            "4                        [start] nous bmes vin [end]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Vocabulary**\n",
        "\n",
        "create a simple vocabulary from the training data"
      ],
      "metadata": {
        "id": "iHfJjDYrdPeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab(sentences):\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        vocab.update(sentence.split())\n",
        "    vocab = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab\n",
        "\n",
        "# Create vocabulary from the training data\n",
        "vocab = create_vocab(train_df['source'].tolist() + train_df['target'].tolist())\n",
        "vocab_size = len(vocab)\n"
      ],
      "metadata": {
        "id": "4B9dng_pdOXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Input Embedding**\n",
        "\n",
        "Initialize the embedding matrix with random values and create functions to get word embeddings"
      ],
      "metadata": {
        "id": "P_qffB0ghC1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 512\n",
        "\n",
        "# Initialize the embedding matrix with random values\n",
        "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")  # Verify shape of embedding matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_yrcvF7dZY9",
        "outputId": "f703768a-4421-4afa-e717-f9d649053e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (35196, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, vocab, embedding_matrix):\n",
        "    idx = vocab.get(word, -1)\n",
        "    if idx == -1:\n",
        "        raise ValueError(f\"Word '{word}' not in vocabulary.\")\n",
        "    return embedding_matrix[idx]\n"
      ],
      "metadata": {
        "id": "T4y6idI7dfRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Positional Encoding**\n",
        "\n"
      ],
      "metadata": {
        "id": "LV2KFOh_p8Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create positional encoding\n",
        "def get_positional_encoding(max_len, embedding_dim):\n",
        "    position = np.arange(max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n",
        "    pos_encoding = np.zeros((max_len, embedding_dim))\n",
        "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "    return pos_encoding"
      ],
      "metadata": {
        "id": "MrWoZTw7pdMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Combine Embeddings and Positional Encodings**\n",
        "\n",
        "combine word embeddings and positional encodings for a given sentence"
      ],
      "metadata": {
        "id": "HS_-4i-5dwEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = train_df['source'].iloc[0].split()\n",
        "sentence_len = len(sentence)\n",
        "print(f\"Sentence: {sentence}, Length: {sentence_len}\")  # Verify sentence and length\n",
        "\n",
        "sentence_embeddings = np.array([get_embedding(word, vocab, embedding_matrix) for word in sentence])\n",
        "print(f\"Sentence Embeddings shape: {sentence_embeddings.shape}\")  # Verify embeddings shape\n",
        "\n",
        "positional_encodings = get_positional_encoding(sentence_len, embedding_dim)\n",
        "print(f\"Positional Encodings shape: {positional_encodings.shape}\")  # Verify positional encodings shape\n",
        "\n",
        "# Add input embeddings and positional encodings\n",
        "input_embedding_with_position = sentence_embeddings + positional_encodings[:sentence_len, :]\n",
        "print(f\"Combined Embedding and Positional Encoding shape: {input_embedding_with_position.shape}\")\n",
        "print(f\"Combined Embedding and Positional Encoding:\\n{input_embedding_with_position}\")"
      ],
      "metadata": {
        "id": "-f2w5f4Vutjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b667e9ca-4df8-4c2a-ef67-cd80266c1dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ['hes', 'bit', 'rough', 'around', 'edges'], Length: 5\n",
            "Sentence Embeddings shape: (5, 512)\n",
            "Positional Encodings shape: (5, 512)\n",
            "Combined Embedding and Positional Encoding shape: (5, 512)\n",
            "Combined Embedding and Positional Encoding:\n",
            "[[ 0.78493689  1.23427811  0.13268065 ...  1.12921899  0.13940888\n",
            "   1.51599795]\n",
            " [ 0.88012215  1.21395799  1.08978388 ...  1.66885422  0.30056578\n",
            "   1.48030144]\n",
            " [ 1.89831024  0.24811682  1.03117638 ...  1.04754841  0.10934511\n",
            "   1.05131897]\n",
            " [ 1.01997201 -0.13534729  0.52024244 ...  1.03105488  0.62201448\n",
            "   1.72469254]\n",
            " [-0.61093644  0.2523216   0.22767455 ...  1.68926288  0.96791953\n",
            "   1.40688246]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOxPsSIJut-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}