{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step by step implementation of Transformer model from the scratch using Numpy**"
      ],
      "metadata": {
        "id": "GglEZxCwnCon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import libraries**\n",
        "\n",
        "libraries for data handling, text preprocessing, and embedding creation"
      ],
      "metadata": {
        "id": "g1p6lD0inVKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure you have the stopwords corpus\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "3A5JHRtisHMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af181cc8-5e44-44d1-8edf-da8e39474da3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data loading and Pre-processing**\n",
        "\n",
        "loading the data from a CSV file and preparing it for training by creating source and target columns"
      ],
      "metadata": {
        "id": "REYnIbPpfE-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = r'/content/sample_data/en-fr.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df['source'] = df['English words/sentences']\n",
        "df['target'] = df['French words/sentences'].apply(lambda x: '[start] ' + x + ' [end]')\n",
        "df = df.drop(['English words/sentences', 'French words/sentences'], axis=1)\n",
        "\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgloTRyrfGj3",
        "outputId": "008f53fd-5eb3-4e02-f315-da1e7bfcd815"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  source                    target\n",
            "0    Hi.      [start] Salut! [end]\n",
            "1   Run!     [start] Cours ! [end]\n",
            "2   Run!    [start] Courez ! [end]\n",
            "3   Who?       [start] Qui ? [end]\n",
            "4   Wow!  [start] Ça alors ! [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle the data and split it into training, validation, and test sets"
      ],
      "metadata": {
        "id": "YS65o3Xoff5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Q_ZFrUdrc9CJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(df) * 0.7)\n",
        "val_size = int(len(df) * 0.2)\n",
        "test_size = int(len(df) * 0.1)\n",
        "\n",
        "print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")  # Check split sizes\n",
        "\n",
        "train_df = df[:train_size]\n",
        "val_df = df[train_size:train_size + val_size]\n",
        "test_df = df[train_size + val_size:]\n",
        "\n",
        "print(f\"Train set size: {len(train_df)}, Validation set size: {len(val_df)}, Test set size: {len(test_df)}\")  # Verify dataset splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc6_AE3Oc-MJ",
        "outputId": "38e461fd-7662-4ea5-b038-19d778bdeaa0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 122934, Val size: 35124, Test size: 17562\n",
            "Train set size: 122934, Validation set size: 35124, Test set size: 17563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the text by converting to lowercase, removing punctuation, and removing stopwords and very short tokens"
      ],
      "metadata": {
        "id": "o7lrLeOvgWR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess sentences to normalize text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    # Convert to lowercase\n",
        "    sentence = sentence.lower()\n",
        "    # Remove punctuation (except for tokens like [start] and [end])\n",
        "    sentence = re.sub(r'[^a-z0-9\\s\\[\\]]', '', sentence)\n",
        "    # Remove stop words and very short tokens\n",
        "    sentence = ' '.join([word for word in sentence.split() if word not in stop_words and len(word) > 2])\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "UcTTm9aIdCuu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "train_df['source'] = train_df['source'].apply(preprocess_sentence)\n",
        "train_df['target'] = train_df['target'].apply(preprocess_sentence)\n",
        "val_df['source'] = val_df['source'].apply(preprocess_sentence)\n",
        "val_df['target'] = val_df['target'].apply(preprocess_sentence)\n",
        "test_df['source'] = test_df['source'].apply(preprocess_sentence)\n",
        "test_df['target'] = test_df['target'].apply(preprocess_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6WOcORZdIQ0",
        "outputId": "07104f62-7d64-41cd-e889-6e0a66be7997"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-b4ff71ef4469>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['source'] = train_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-57-b4ff71ef4469>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['target'] = train_df['target'].apply(preprocess_sentence)\n",
            "<ipython-input-57-b4ff71ef4469>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df['source'] = val_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-57-b4ff71ef4469>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df['target'] = val_df['target'].apply(preprocess_sentence)\n",
            "<ipython-input-57-b4ff71ef4469>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['source'] = test_df['source'].apply(preprocess_sentence)\n",
            "<ipython-input-57-b4ff71ef4469>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['target'] = test_df['target'].apply(preprocess_sentence)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())  # Verify preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKfVTxl-gmJj",
        "outputId": "c479fc7e-7704-458c-e62e-e96242ded727"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      source  \\\n",
            "0                          tom might traitor   \n",
            "1                                      agony   \n",
            "2  would sit hours reading detective stories   \n",
            "3                working report day tomorrow   \n",
            "4                   dont always obey parents   \n",
            "\n",
            "                                              target  \n",
            "0              [start] tom pourrait tre tratre [end]  \n",
            "1                        [start] jtais lagonie [end]  \n",
            "2  [start] sassirait des heures lire des histoire...  \n",
            "3  [start] travaillerai mon rapport toute journe ...  \n",
            "4  [start] ils nobissent pas toujours leurs paren...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Vocabulary**\n",
        "\n",
        "create a simple vocabulary from the training data"
      ],
      "metadata": {
        "id": "iHfJjDYrdPeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab(sentences):\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        vocab.update(sentence.split())\n",
        "    special_tokens = ['[start]', '[end]', '[pad]', '[unk]']\n",
        "    vocab.update(special_tokens)\n",
        "    vocab = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab\n",
        "\n",
        "vocab = create_vocab(train_df['source'].tolist() + train_df['target'].tolist())"
      ],
      "metadata": {
        "id": "4B9dng_pdOXd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Input Embedding**\n",
        "\n",
        "Initialize the embedding matrix with random values and create functions to get word embeddings"
      ],
      "metadata": {
        "id": "P_qffB0ghC1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 512\n",
        "\n",
        "def initialize_embeddings(vocab, embedding_dim):\n",
        "    vocab_size = len(vocab)\n",
        "    embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = initialize_embeddings(vocab, embedding_dim)\n"
      ],
      "metadata": {
        "id": "Z_yrcvF7dZY9"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, vocab, embedding_matrix):\n",
        "    idx = vocab.get(word, vocab['[unk]'])  # Use [unk] token for unknown words\n",
        "    return embedding_matrix[idx]"
      ],
      "metadata": {
        "id": "T4y6idI7dfRE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Positional Encoding**\n",
        "\n"
      ],
      "metadata": {
        "id": "LV2KFOh_p8Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create positional encoding\n",
        "def get_positional_encoding(max_len, embedding_dim):\n",
        "    position = np.arange(max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n",
        "    pos_encoding = np.zeros((max_len, embedding_dim))\n",
        "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "    return pos_encoding"
      ],
      "metadata": {
        "id": "MrWoZTw7pdMv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Combine Embeddings and Positional Encodings**\n",
        "\n",
        "combine word embeddings and positional encodings for a given sentence"
      ],
      "metadata": {
        "id": "HS_-4i-5dwEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = train_df['source'].iloc[0].split()\n",
        "sentence_len = len(sentence)\n",
        "print(f\"Sentence: {sentence}, Length: {sentence_len}\")  # Verify sentence and length\n",
        "\n",
        "sentence_embeddings = np.array([get_embedding(word, vocab, embedding_matrix) for word in sentence])\n",
        "print(f\"Sentence Embeddings shape: {sentence_embeddings.shape}\")  # Verify embeddings shape\n",
        "\n",
        "positional_encodings = get_positional_encoding(sentence_len, embedding_dim)\n",
        "print(f\"Positional Encodings shape: {positional_encodings.shape}\")  # Verify positional encodings shape\n",
        "\n",
        "# Add input embeddings and positional encodings\n",
        "input_embedding_with_position = sentence_embeddings + positional_encodings[:sentence_len, :]\n",
        "print(f\"Combined Embedding and Positional Encoding shape: {input_embedding_with_position.shape}\")\n",
        "print(f\"Combined Embedding and Positional Encoding:\\n{input_embedding_with_position}\")"
      ],
      "metadata": {
        "id": "-f2w5f4Vutjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7bec6e-7aa0-47b4-d515-50d6d75b9cd5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ['tom', 'might', 'traitor'], Length: 3\n",
            "Sentence Embeddings shape: (3, 512)\n",
            "Positional Encodings shape: (3, 512)\n",
            "Combined Embedding and Positional Encoding shape: (3, 512)\n",
            "Combined Embedding and Positional Encoding:\n",
            "[[ 0.91440777  1.97063465  0.40761718 ...  1.06115982  0.1081431\n",
            "   1.46614788]\n",
            " [ 1.20990732  0.62919128  1.42647248 ...  1.87951286  0.77585246\n",
            "   1.75267445]\n",
            " [ 1.23112107 -0.23051104  1.82407744 ...  1.52251075  0.37818941\n",
            "   1.05917547]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOxPsSIJut-X"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder**"
      ],
      "metadata": {
        "id": "1EsPNtr5R_Cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer normalization layer"
      ],
      "metadata": {
        "id": "uyGEKfcGCfy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_normalization(x, epsilon=1e-6):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    normalized_x = (x - mean) / np.sqrt(variance + epsilon)\n",
        "    return normalized_x"
      ],
      "metadata": {
        "id": "zjdZfJ0jSCQZ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-head Self Attention"
      ],
      "metadata": {
        "id": "sam3Ot29GI2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_head_attention(query, key, value, num_heads):\n",
        "    d_k = query.shape[-1]  # Dimensionality of the key/query/value\n",
        "    assert d_k % num_heads == 0\n",
        "    depth = d_k // num_heads  # Depth of each head\n",
        "\n",
        "    # Split the embedding into multiple heads\n",
        "    def split_heads(x):\n",
        "        seq_len, d_model = x.shape\n",
        "        new_shape = (seq_len, num_heads, depth)\n",
        "        x = x.reshape(new_shape)\n",
        "        x = x.transpose(1, 0, 2)  # (num_heads, seq_len, depth)\n",
        "        return x\n",
        "\n",
        "    query = split_heads(query)\n",
        "    key = split_heads(key)\n",
        "    value = split_heads(value)\n",
        "\n",
        "    # Scaled Dot-Product Attention\n",
        "    def scaled_dot_product_attention(q, k, v):\n",
        "        matmul_qk = np.matmul(q, k.transpose((0, 2, 1)))  # (num_heads, seq_len, seq_len)\n",
        "        dk = q.shape[-1]\n",
        "        scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "        attention_weights = np.exp(scaled_attention_logits - np.max(scaled_attention_logits, axis=-1, keepdims=True))\n",
        "        attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "        output = np.matmul(attention_weights, v)\n",
        "        return output\n",
        "\n",
        "    attention_output = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "    # Concatenate heads\n",
        "    def concatenate_heads(x):\n",
        "        num_heads, seq_len, depth = x.shape\n",
        "        new_shape = (seq_len, num_heads * depth)\n",
        "        x = x.transpose(1, 0, 2).reshape(new_shape)\n",
        "        return x\n",
        "\n",
        "    attention_output = concatenate_heads(attention_output)\n",
        "    return attention_output"
      ],
      "metadata": {
        "id": "ozxgYG5DGCK_"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward Network"
      ],
      "metadata": {
        "id": "GrvV_h5dGQ7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(x, d_ff, embedding_dim):\n",
        "    W1 = np.random.randn(embedding_dim, d_ff)\n",
        "    b1 = np.random.randn(d_ff)\n",
        "    W2 = np.random.randn(d_ff, embedding_dim)\n",
        "    b2 = np.random.randn(embedding_dim)\n",
        "\n",
        "    def feed_forward(x):\n",
        "        x = np.maximum(0, np.matmul(x, W1) + b1)  # ReLU activation\n",
        "        x = np.matmul(x, W2) + b2\n",
        "        return x\n",
        "\n",
        "    return feed_forward(x)\n"
      ],
      "metadata": {
        "id": "sBdlzeIlGQHu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder block"
      ],
      "metadata": {
        "id": "hdda7lHnGg1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(x, num_heads, d_ff, embedding_dim, epsilon=1e-6):\n",
        "    # Self-Attention\n",
        "    attention_output = multi_head_attention(x, x, x, num_heads)\n",
        "    attention_output += x  # Residual connection\n",
        "    attention_output = layer_normalization(attention_output, epsilon)  # Add & Normalize\n",
        "\n",
        "    # Feed-Forward Network\n",
        "    ff_output = feed_forward_network(attention_output, d_ff, embedding_dim)\n",
        "    ff_output += attention_output  # Residual connection\n",
        "    ff_output = layer_normalization(ff_output, epsilon)  # Add & Normalize\n",
        "\n",
        "    return ff_output\n"
      ],
      "metadata": {
        "id": "GylUO-dbGNOy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder stack"
      ],
      "metadata": {
        "id": "0wpa8tG8Go15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_stack(x, num_layers, num_heads, d_ff, embedding_dim, epsilon=1e-6):\n",
        "    for _ in range(num_layers):\n",
        "        x = encoder_layer(x, num_heads, d_ff, embedding_dim, epsilon)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "sXv7UuWuGmPN"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # test\n",
        "# num_layers = 6\n",
        "# num_heads = 8\n",
        "# d_ff = 2048\n",
        "# embedding_dim = 512\n",
        "\n",
        "# batch_size = 1\n",
        "# sentence_length = 10\n",
        "# sentence_embeddings = np.random.rand(batch_size, sentence_length, embedding_dim)\n",
        "\n",
        "# # encoder stack with 6 encoder blocks\n",
        "# encoded_output = encoder_stack(sentence_embeddings, num_layers, num_heads, d_ff, embedding_dim)\n",
        "# print(f\"Encoded Output after 6 encoder layers:\\n{encoded_output}\")\n"
      ],
      "metadata": {
        "id": "UEjZ6i2fGrS8"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RnTuvJTHIXqx"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoder**"
      ],
      "metadata": {
        "id": "IsTAaucXLBzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder layer\n",
        "def decoder_layer(x, encoder_output, num_heads, d_ff, embedding_dim, epsilon=1e-6):\n",
        "    # Masked Self-Attention\n",
        "    masked_attention_output = multi_head_attention(x, x, x, num_heads)\n",
        "    masked_attention_output += x  # Residual connection\n",
        "    masked_attention_output = layer_normalization(masked_attention_output, epsilon)  # Add & Normalize\n",
        "\n",
        "    # Encoder-Decoder Attention\n",
        "    attention_output = multi_head_attention(masked_attention_output, encoder_output, encoder_output, num_heads)\n",
        "    attention_output += masked_attention_output  # Residual connection\n",
        "    attention_output = layer_normalization(attention_output, epsilon)  # Add & Normalize\n",
        "\n",
        "    # Feed-Forward Network\n",
        "    ff_output = feed_forward_network(attention_output, d_ff, embedding_dim)\n",
        "    ff_output += attention_output  # Residual connection\n",
        "    ff_output = layer_normalization(ff_output, epsilon)  # Add & Normalize\n",
        "\n",
        "    return ff_output\n",
        "\n",
        "def decoder_stack(x, encoder_output, num_layers, num_heads, d_ff, embedding_dim, epsilon=1e-6):\n",
        "    for _ in range(num_layers):\n",
        "        x = decoder_layer(x, encoder_output, num_heads, d_ff, embedding_dim, epsilon)\n",
        "    return x"
      ],
      "metadata": {
        "id": "gsXfvkWjLFId"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer**"
      ],
      "metadata": {
        "id": "iA3YousMerj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer:\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_ff, embedding_dim, vocab_size, max_len, epsilon=1e-6):\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
        "        self.positional_encodings = get_positional_encoding(max_len, embedding_dim)\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        sentence_len = len(sentence)\n",
        "        sentence_embeddings = np.array([get_embedding(word, vocab, self.embedding_matrix) for word in sentence])\n",
        "        input_embedding_with_position = sentence_embeddings + self.positional_encodings[:sentence_len, :]\n",
        "        encoded_output = encoder_stack(input_embedding_with_position, self.num_encoder_layers, self.num_heads, self.d_ff, self.embedding_dim, self.epsilon)\n",
        "        return encoded_output\n",
        "\n",
        "    def decode(self, sentence, encoder_output):\n",
        "        sentence_len = len(sentence)\n",
        "        sentence_embeddings = np.array([get_embedding(word, vocab, self.embedding_matrix) for word in sentence])\n",
        "        input_embedding_with_position = sentence_embeddings + self.positional_encodings[:sentence_len, :]\n",
        "        decoded_output = decoder_stack(input_embedding_with_position, encoder_output, self.num_decoder_layers, self.num_heads, self.d_ff, self.embedding_dim, self.epsilon)\n",
        "        return decoded_output\n",
        "\n",
        "    def forward(self, source_sentence, target_sentence):\n",
        "        encoder_output = self.encode(source_sentence)\n",
        "        decoder_output = self.decode(target_sentence, encoder_output)\n",
        "        return decoder_output"
      ],
      "metadata": {
        "id": "rzWcQE4pesD3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "embedding_dim = 512\n",
        "vocab_size = len(vocab)\n",
        "max_len = 100  # Assuming max length of the sentence\n",
        "\n",
        "# Create transformer model instance\n",
        "transformer = Transformer(num_encoder_layers, num_decoder_layers, num_heads, d_ff, embedding_dim, vocab_size, max_len)\n",
        "\n",
        "# Test with a sample sentence\n",
        "source_sentence = train_df['source'].iloc[0].split()\n",
        "target_sentence = train_df['target'].iloc[0].split()\n",
        "\n",
        "# Forward pass through the transformer model\n",
        "output = transformer.forward(source_sentence, target_sentence)\n",
        "print(f\"Transformer model output:\\n{output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9fu9vbknDdk",
        "outputId": "086a701b-d5fb-4649-bcc6-de204c9a8ad6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer model output:\n",
            "[[ 1.80158113  0.11364658  0.56420411 ... -2.19337955  2.18923926\n",
            "  -0.93490039]\n",
            " [ 1.80388736  0.1131103   0.56379054 ... -2.19225459  2.18726508\n",
            "  -0.93634658]\n",
            " [ 1.80623934  0.11188299  0.56006097 ... -2.18851046  2.19022638\n",
            "  -0.93444424]\n",
            " [ 1.8064679   0.11245047  0.55633796 ... -2.19061637  2.18666444\n",
            "  -0.9327703 ]\n",
            " [ 1.80898632  0.10891185  0.55694253 ... -2.19065296  2.18481266\n",
            "  -0.93418914]\n",
            " [ 1.81067434  0.1085136   0.55440942 ... -2.18875003  2.1846308\n",
            "  -0.934971  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(transformer, source_sentence, vocab, max_len=100):\n",
        "    # Prepare the source sentence\n",
        "    source_tokens = source_sentence.split()\n",
        "\n",
        "    # Encode the source sentence\n",
        "    encoder_output = transformer.encode(source_tokens)\n",
        "\n",
        "    # Initialize target sentence with the start token\n",
        "    target_sentence = ['[start]']\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Prepare the target input for the decoder\n",
        "        target_tokens = target_sentence[-(max_len - 1):]  # Ensure max length is respected\n",
        "        target_tokens_padded = target_tokens + ['[pad]'] * (max_len - len(target_tokens) - 1)\n",
        "\n",
        "        # Decode to get the next token probabilities\n",
        "        decoder_output = transformer.decode(target_tokens_padded, encoder_output)\n",
        "\n",
        "        # Get the predicted token\n",
        "        next_token_probs = decoder_output[-1]  # Last timestep output\n",
        "        predicted_token_idx = np.argmax(next_token_probs, axis=-1)\n",
        "\n",
        "        # Convert index to token\n",
        "        inverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "        predicted_token = inverse_vocab.get(predicted_token_idx, '[unk]')\n",
        "\n",
        "        if predicted_token == '[end]':\n",
        "            break\n",
        "\n",
        "        target_sentence.append(predicted_token)\n",
        "\n",
        "    return ' '.join(target_sentence[1:])  # Exclude the [start] token\n",
        "\n",
        "# Example source sentence\n",
        "source_sentence = \"tom hiding something\"  # Example source sentence\n",
        "translated_sentence = translate(transformer, source_sentence, vocab)\n",
        "print(f\"Source Sentence: {source_sentence}\")\n",
        "print(f\"Translated Sentence: {translated_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ndT2Ld2nKXS",
        "outputId": "a9ca824f-020d-48a7-f604-dab89c3dae13"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentence: tom hiding something\n",
            "Translated Sentence: repainted 112 cra flip rendezvous contains bienaime nextdoor stress physicians origine prcurseur finale werewolves phare cramique carelessly cderont years stay ouvrant captur consigne helmets rases pal compagnie nextdoor modernes touille niece nchapperas verbes financirement baromtre survivrai contributing avare compltes words coutume crit ouvrant fabriqus lastrologie chanteraistu imply lid cruaut ttre paroles chanteraistu cra laborer obirez oversleep transferred rver fisc counter vehemently payerons reprogramm contains ntre plaisantezvous drawing regardiezvous dimaginer devoid fins twinkle drapeaux assigned shifting faille violences racle renards battage particles get veng ncessite nul ceo finale compltes leve dextinction entretenues chienne ground plage lenclos leve entretenues oreillons taitelle 112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Dn-y00plWyE"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}